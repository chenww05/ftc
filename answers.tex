\documentclass{letter}
\usepackage{graphicx}
\usepackage{color}
\date{Feb 15, 2015}

\newenvironment{review}%          environment name
{\textbf{Reviewer comment:}\begin{quote}}% begin code
{\end{quote}}%  

\newcommand{\todo}[1]{%                                                                                                                            
      \color{red}\textbf{[TODO]} #1\color{black}}

\newcommand{\answer}[1]{%                                                                                                                            
      \textbf{Answer:} #1}

\usepackage{hyperref}

\newcommand{\revised}[1]{\emph{#1}\color{black}}
\newcommand{\rev}[1]{\color{blue} #1\color{black}}

\begin{document}

\begin{letter}{}

\opening{Dear Editor and Reviewers,}

We would like to thank you for your thorough review and the useful
comments and suggestions formulated about our paper ``Dynamic and Fault-Tolerant Clustering for Scientific Workflows'' submitted to
IEEE Transactions on Cloud Computing.

The remainder of this letter contains our answers to your reviews. 

We thank you again for your valuable review, and we remain available for any further information you may need.

\vspace{0.5cm}

Sincerely yours,

\vspace{1cm}

Weiwei Chen, Rafael Ferreira da Silva, Ewa Deelman, Thomas Fahringer

\newpage

%
% Reviewer 1
%
\textbf{Reviewer 1}

\begin{review}
While Pegasus represents one good use case for this work, there are other systems based on different approaches like glideinWMS or Koala that strive to improve scheduling (and job throughput in particular) in faulty environments. The larger audience, including the Grid domain, will greatly benefit from comparisons with other scheduling strategies (e.g., based on static resource allocations). 
\end{review}

\answer{Our work is mainly focused on the optimization of workflow executions on distributed systems. Although there are several systems, such as glideinWMS or Koala, that provide fault-tolerance scheduling, they cannot optimize, for example, the workflow engine delay (the time span between the job release and its submission to the scheduler), which can be up to 100\% for very short runtime tasks (see Table 1). Our methods do not aim to replace these system schedulers, but to provide graph optimizations for the workflow execution regardless of the underlying scheduling system.}

\revised{}



\begin{review}
Regarding the analysis, the paper uses mostly the makespan (a definition would be also good) for drawing the conclusions. I consider there are other important metrics to analyze for such strategies, especially from a scheduling point of view, otherwise the analysis seems geared towards achieving users' objectives without taking in consideration resource utilization or other objectives.
\end{review}

\answer{We added a definition of makespan to the introduction section of the paper. We do not aim at measuring resource utilization since high resource utilization may imply in poor clustering (task re-computation). However, we measure the amount of tasks re-computed for each strategy. The measurements are presented in Table 4.}



\begin{review}
How does such an approach compare to Condor's matchmaking and its harnessing approach of wasted CPU power from otherwise idle desktop workstations?
\end{review}

\answer{We added a paragraph in Related Work to discuss the difference and related scheduling algorithms.}



\begin{review}
The scheduling objectives of the research should be better clarified in the abstract. Also, the bibliography and references should include more concrete examples, as the Grid domain would benefit from this body of work. 
\end{review}

\answer{We added a paragraph in Related Work to discuss related scheduling algorithms.}



\newpage

%
% Reviewer 2
%
\textbf{Reviewer 2}

\begin{review}
I'd like to see more quantified clarification on the ``scheduling overhead'' and ``workflow engine delay''. Intuitively, or based on my experience, the scheduling overhead should be trivial in practice, especially compared with the data movement overhead between task stages. In this paper, it emphasizes the ``scheduling overhead'' by just citing some previous work of the same authors. It would be better to put some numbers here to make this statement more convincing. 
\end{review}

\answer{We added a table (Table 1) that shows the average workflow engine and queue delays for 3 scientific workflow applications executed on a distributed platform.}



\begin{review}
In Page 3, the formula for $runtime_l$, $t_2$ and $t_3$ have no data dependency, they should be easily parallelized. However they are assumed for only sequential execution in order to make the formulator $runtime_l$ holds. It would be good to provide more use cases to justify the sequential assumption.
\end{review}

\answer{We agree that since $t_2$ and $t_3$ have no data dependency, they could be run in parallel. However, this is true only when $t_2 > s_2$ and $t_3 > s_3$, which is not always the case as shown in Table 1. We updated the text to compute the workflow on Fig. 3 (left) as $runtime_l= s_1+t_1+\max(s_2,s_3)+\max(t_2,t_3)+s_4+t_4$, and updated the condition that $runtime_l > runtime_r$ as long as $c_1 < \max(s_2,s_3)$.}




\begin{review}
``the default network bandwidth is 15MB'', I was wondering if in the simulation, each data transfer can achieve this bandwidth or simultaneous transfers will compete the 15MB bandwidth and will have some slowdowns?
\end{review}

\answer{The network bandwidth of 15MB is defined for each data transfer. We added the following sentence to clarify our assumption:}

\revised{In our execution model, the network bandwidth is the maximum allowed data transfer speed between a pair of virtual machines per file.}



\begin{review}
I'd like to see more description on the workflow execution traces instead of just citing two papers.
\end{review}

\answer{The traces are used to feed the Workflow Generator toolkit to create synthetic workflows. Due to space constraint, we provide a summary of the main characteristics of these traces on Tables 1 (added in this revised version) and 3. In addition, we provided a reference (\url{http://workflowarchive.org}) to where all traces, profile data, and the workflow generator are freely available.}



\begin{review}
The experiments are conducted against five different workflows. I'd like to see some generic comparison of these workflows. That is, what algorithms are good for what kind of workflow. A summary would be good at the end of the experimental results.
\end{review}

\answer{The Dynamic Reclustering algorithm outperforms most of the methods for the 5 workflow applications, however, it yields poorer 
performance when the workflow structure is irregular (e.g., SIPHT workflow). We added a paragraph to the end of the experiments section to summarize the experimental results.}



\newpage

%
% Reviewer 3
%
\textbf{Reviewer 3}

\begin{review}
One concern is that it might be difficult to apply these techniques in a practical system.  The authors collect quite a bit of data and use numerical optimization to obtain the task cluster size at regular intervals and that might be expensive, however, they consider relatively small clusters (20 CPUs) so it is probably tractable at that scale.  It would be interesting to hear more about how this would connect to a real workflow management system but the paper is self-contained given the space constraint. 
\end{review}

\answer{We added a reference to show that such setting is consistent with many real execution experiments in Section 4.2:}

\revised{This assumption is also consistent with the setting of many real execution experiments [63]. }

Due to the space constraint we are not able to provide an experiment where we vary the number of virtual machines (VMs) to represent large-scale execution. However, in a recent work [Chen-2014], we demonstrated that task clustering techniques have similar performance for small- (20 VMs) and large-scale (1800 VMs) scenarios.

[Chen-2014] W. Chen, R. Ferreira da Silva, E. Deelman, R. Sakellariou, Using imbalance metrics to optimize task clustering in scientific workflow executions, Future Generation Computer Systems, http://dx.doi.org/10.1016/j.future.2014.09.014.






\newpage

%
% Reviewer 4
%
\textbf{Reviewer 4}

\begin{review}
The paper is written in easy to understand practical terms, except Section 2 which contrasts with the rest of the paper through its rather theoretical nature. The connection between Section 2 and the rest of the paper is not really obvious and it took me several iterations of the paper to understand it. I suggest that the authors increase the cross references to better highlight it. 
\end{review}

\answer{In section 4, we increased the cross references to highlight the use of the conclusions of Section 2.}



\begin{review}
The drawback with this paper in my opinion is the evaluation done using some mixture of ``simulated real-world workflows''. While I acknowledge the importance of simulation in validating such heuristics, I am not convinced that one of its main characteristics, the reproducibility, is guaranteed in this paper. The workflow traces injected in the workflow simulator do not seem to be open data. In general, I favour a complete separation between complete synthetic simulation, and its validation in isolated real-world scenarios, but through real execution. 
\end{review}

\answer{Synthetic workflows are computer-generated workflows that have characteristics derived from real workflow applications. They are similar to real workflows, but instead of specifying real computations, they specify artificial resource requirements, including the runtime and memory usage of tasks, and the sizes of input and output files. Synthetic workflows are generated from synthetic workflow generators, which use randomization and parameter sweeps to create many different synthetic workflows using the same set of input parameters. We added a reference to the generator as well as the traces, profile data, and characterizations:}

\revised{Traces, profile data, and characterizations are freely available online for the community [65]. }



\begin{review}
On the other hand, the authors do not aim to analyse the results with respect to workflow structure and topological characteristics, which makes the reusability of the results in different contexts difficult to assess. Table 2 contains very superficial data in this direction. I suggest that the authors add a discussion section where they take a step back and try to formulate some general recommendations that can be used by practitioners for running different workflows in the Cloud.
\end{review}

\answer{Due to the space constraint, we added to the end of Section 4.3 a brief discussion on general recommendations based on the workflow structure and the proposed strategies:}

\revised{Our experimental results show that adaptive task re-clustering methods outcomes better performance than simple static methods that retry failed clustered jobs. However, the performance gain may be significantly impacted if the job clustering size is not adjusted to the nearly optimal size. The Dynamic Reclustering algorithm outperforms most of the methods for the 5 workflow applications, however, it yields poorer 
performance when the workflow structure is irregular (e.g., SIPHT workflow). In this case, Vertical Reclustering would be more suitable.}



\begin{review}
In this sense, I have doubts that the five workflows cover all patterns encountered today in scientific workflows. Adding some synthetic workflows to the simulation could strengthen the completeness of the evaluation.
\end{review}

\answer{We believe the simulated workflow applications used in this work represent a reasonable large set of workflow patterns found in literature:}

\revised{Cerezo, Nadia. Workflows conceptuels. Diss. Université Nice Sophia Antipolis, 2013.}

\revised{Cordasco, Gennaro, and Arnold L. Rosenberg. "On scheduling series-parallel DAGs to maximize AREA." International Journal of Foundations of Computer Science 25.05 (2014): 597-621.}

\revised{Workflow Patterns, http://www.workflowpatterns.com}

\revised{Pegasus Applications, https://pegasus.isi.edu/applications}



\begin{review}
With respect to style, the paper contains too many self-references and some captions in figures are too small.
\end{review}

\answer{We increased the caption of several figures, and reduced the self-references on the text.}



\end{letter}
\end{document}